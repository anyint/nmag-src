#!/usr/bin/env nsim

"""

New design idea (May 2008):

For a number of configurations ('id's) we have a number of different fields stored. 

Different commands can operate on different ids and different fields

Need data structure that stores

 - initially all the ids and associated fields that are stored in the file.

   A possible structure could be a dictionary with the ids being the
   keys and a list of (alphabetically sorted) fieldnames as the
   values.

 - reduce the ids (using the --range command)

 - reduce the number of fields (using the --fields command)

Then deal with --dump and --vtk, iterating through ids and fields that have been selected.

Have just implemented this (fangohr 30/05/2008)

"""


import optparse,sys,types,os

import logging
log = logging
logging.getLogger('').setLevel(logging.INFO)

import nfem.hdf5_v01
import nfem.visual

from nsim.si_units import SI

import numpy

def parse_range_string(rangestring,ids):
    #convert range string into list
    try:
        rangelist = eval(rangestring)
    except StandardError,msg:
        doc = "Something went wrong when parsing your '--range' string.\n"
	doc +="The code that couldn't be parsed is '%s'.\n" % rangestring
        doc +="You need to specify working Python code. Some examples are:\n"
        doc +="--range 17            #will select 17\n"
        doc +='--range "range(5,10)" #will select [5,6,7,8,9]\n'
        doc +='--range "[2,5,10,42]" #will select [2,5,10,42]\n'
        doc +='--range "range(10)+[20,25,31,42]" #...\n'
	doc +='The error message is:"%s"' % str(msg)
        raise StandardError,doc
    if type(rangelist) in [types.IntType,types.LongType]  and rangelist >= 0:
        rangelist = [rangelist]
    return rangelist

def parse_fields_string(fieldstring): 

    """Receive string from command
    line (such as "phi,m" or [phi,m] or ["phi","m"]) and return list
    of field strings such as ['phi','m']
    """

    log.debug("parse_fields_string: input '%s'" % fieldstring)
    #We allow square brackets and parentheses, i.e. acceptable field specifications are [m,phi] or (m,phi)
    fields = fieldstring.lstrip('[').rstrip(']').lstrip('(').rstrip(')').split(',')
    #strip off any single or double quotes
    fields = map( lambda field : field.lstrip("'"), fields)
    fields = map( lambda field : field.rstrip("'"), fields)
    fields = map( lambda field : field.lstrip('"'), fields)
    fields = map( lambda field : field.rstrip('"'), fields)

    log.debug("parse_fields_string: converted to list: fields='%s'" % fields)

    return fields
    
def get_unit_obj_and_si_conversionfactor( f, si_unit, output_unit ):
    """Return unit (Physical) Object and SI scaling factor for SI/su units

    :Parameters:
      `f` : hd5 file handler
        The open hdf5 file.

      `si_unit` : Physical Object
        The SI unit of the data (i.e. for distances, this is always SI(1,'m'))

      `output_unit` : either 'si' or 'su'
        If ``si`` is chosen then data is expressed in SI units and the scaling factor is 1.0.

        If ``su`` is chosen, then the data is expressed in simulation
        units that correspond to the SI unit provided. The scaling
        factor is the number that needs to be multiplied with the SI
        unit value to obtain the simulation unit value.

    :Returns:
      (unit,si_conversion_factor)
        where unit is a ``Physical`` object and ``si_conversion_factor`` is a float.

    """
    if output_unit == 'si':
        unit = si_unit
        si_conversion_factor = 1.0
    elif output_unit == 'su':
        #get simulation units 
        from nsim.su_units import SimulationUnits
        su = nfem.hdf5_v01.get_su_units(f)
        unit = su.conversion_factor_of(si_unit)
        si_conversion_factor = su.of(si_unit)
    else:
        raise StandardError,"output_unit=%s -- internal error" % output_unit

    return unit,si_conversion_factor


def _unit_conversion_helper( fieldname, subfieldname, sidata, sibase, output_unit, options):
    """Helping function to do conversion of fields into si or su units.

    This is called repeatedly (twice by vtk writing routine, once by dump_ascii).

    In future, we may provide extra scaling instructions via commond line
    parameters and could deal with those here.

    :Parameters:

     `data` : numpy array
        The data (in SI units)

     `sibase` : SI object
        This is the fundamential dimension for the units for this
        field. For example, for distances this would be SI(1,'m').

     `options` : optparse-options object
        This is the options object as obtained from optparse
        (By passing this potentiall complex object down here, we can easily
         extend functionality.)
         This is not currently used but maybe later (for example to
         scale particular field/subfield in a given way.)

    :Returns:
      `data` : numpy array
         scaled to users wishes (usually SI or simulation units)

      `unit` : SI object
         providing the multiplyer for the data to get to SI units

    """

    unit,factor=get_unit_obj_and_si_conversionfactor( fh, sibase, output_unit )

    data = sidata*factor

    return data,unit

def _unit_meshposition_conversion_helper(fh, options, supos):
    """

    :Parameters:
      `fh` : filehandler
        for the hdf5 files

     `supos` : numpy array
        mesh coordinates in simulation units

     `options` : optparse-options object
        This is the options object as obtained from optparse.
        In particular, this contains the 'posfactor' option if given
        by the user which we use to express the mesh coordinates
        accordingly (i.e. in units of this posfactor times metre).
    """
    #this is the unit length associated with the stored mesh,
    #i.e. this length multiplied by the mesh coordinates gives metres.

    mesh_unit_length = nfem.hdf5_v01.get_mesh_unit(fh).value

    if options.posfactor:
        log.debug("Scaling mesh coordinates such that mesh"+\
                  "coordinate*%g is the position in metres" %\
                  (float(options.posfactor)))
        #if the user proides a posfactor, then
        posunit = SI(options.posfactor,'m')
        if float(options.posfactor) == 0.0:
            raise "--posunit has to be non-zero"
        #convert coordinates into meters, and then divide by posfactor
        pos = supos * mesh_unit_length / float(options.posfactor)
    else:
        #otherwise return raw coordinates untouched
        posunit = SI(mesh_unit_length,'m')
        pos = supos

    return pos, posunit


def dump_field_ascii_1_ts(fh,field,uid,output_unit,options):
    """The name stands for DUMP (one) FIELD in ASCII for 1 TimeStep.

    :Parameters:
      `fh` : filehandler
        the hdf5 file

      `field`: string
        name of the field

      `id` : integer
        identifier for the data set to be printed

      `output_unit` : string ('si' or 'su')
        whether to output field data in si or su units

      `options` : options object from optparse
        (providing flexibility in passing more options to output
        functions).

    """

    def str_list_as_vec(li):
        if type(li) in [types.IntType,types.FloatType,numpy.typeDict['float32']]:
            return "%12g" % li
        elif len(li) == 1:
            return "%12g" % li
        else:
            return '('+" ".join(map(lambda x :"%12g" % x, li))+' )'
    
    row,stage,step,sitime = nfem.hdf5_v01.get_row_stage_step_time_for_field_and_id(fh,field,uid)

    #######

    table = fh.getNode(fh.root.data.fields,field)

    dofnames = [name for name in table.colnames if not name in ['time','step','stage','id']]

    for dofname in dofnames:
        #positions come in simulation units (su), data in si
        supos,sidata,site = nfem.hdf5_v01.get_dof_row_data(fh,field,dofname,row)

        print "field    : %s" % field
        print "subfield : %s" % dofname

        time_si_unit = nfem.hdf5_v01.get_time_unit(fh)
        time,time_unit = _unit_conversion_helper( 'time', 'time', sitime, time_si_unit, output_unit, options)

        print "time     : %g * %s" % (time,time_unit)
        print "id       : %d" % (uid)
        print "step     : %d" % (step)	
        print "stage    : %d" % (stage)
        
        #get si_unit for this dof 
        sibase = eval(nfem.hdf5_v01.get_units_by_dofname(fh)[dofname])
        data,unit = _unit_conversion_helper( field, dofname, sidata, sibase, output_unit, options)

	print "field unit: %s" % (str(unit))
        
        #The mesh is the only data type not (necessarily) stored in SI
        #units. We also leave it to the user to decide in what units to
        #express it (--posunit).
        #Here we go:

        pos, posunit = _unit_meshposition_conversion_helper(fh,
                                                            options,
                                                            supos)


	print "position unit: %s" % (str(posunit))

        print "row: %d" % row
        print "#Start (index in h5 table, dofsite, pos, data)"
        
	#from IPython.Shell import IPShellEmbed
    	#ipshell = IPShellEmbed([])
    	#ipshell()


	if nfem.hdf5_v01.pytables_master_version() == 1:
	        assert sitime == table[row].field('time'),"Internal error in nmagpp"
	else:
	        assert sitime == table.read(start=row,stop=row+1,field='time'),"Internal error in nmagpp"

        assert len(data) == len(pos), "len(pos) and len(data) doesn't match"

	def site_id(s):
            if type(s) == types.IntType: # defensive coding, we actually expect a 1-tuple,
                                         # but we allow this to be given as an int.
                return "%d" % s
            elif len(s) == 1:
                return "%d" % s
            else:
                return '('+",".join(map(lambda x :"%d" % x, s))+')'

        for i,dofsite,point,datum in map(None,range(len(pos)),site,pos,data):
            print "%4d %10s %s %s" % (i,site_id(dofsite),str_list_as_vec(point),str_list_as_vec(datum))
        print "#End"


def dump_fields_ascii(fh,fields_by_id,output_unit,options):
    log.debug("dump_fields_ascii: Entering for fields_by_id=%s" % str(fields_by_id))
    ids = fields_by_id.keys()
    ids.sort()
    for id in ids:
        for field in fields_by_id[id]:
            dump_field_ascii_1_ts(fh,field,id,output_unit,options)


def parse_command_line(argvs):
    usage="""

    %prog [options] nsimfile_dat.h5 [outputfile]

    (C) University of Southampton, United Kingdom, 2005,2006,2007,2008

    The NMAG PostProcessing (nmagpp) tool.

    Overview
    --------

    This program is used to analyse the _dat.h5 files that nmag produces.

    Each _dat.h5 file contains spatially resolved data for
 
     - a number of different configurations (each of those has a unique
       `id`; this id is the same as given in the ndt file). A configuration
       is (nearly) the same as the fielddata for a given time step. 

     - and (potentially) for each id a number of different fields.

    While it is common to always save the same fields (say, to save
    only the magnetisation 'm' field again and again), it is also
    possible for the user to save different fields for different ids.

    Exporting data from the data file
    ---------------------------------

    By default, nmagpp, will work through all ids and all fields and
    carry out some action. The two most useful actions are:

     --dump : this will print all the field dato to stdout 

     --vtk VTKFILENAME : this will convert all fields for a given id
                         into a vtk file.

    The user can reduce the number of fields that are processed in
    this way by using the `--fields` switch. For example, to only
    process the 'm' field, use `--fields m` or to process the m and
    E_demag field, use `--fields m,E_demag`.

    The user can further reduce the number of ids that are processed using the
    --range switch (see below for usage).

    The way the numerical data is printed or plotted can be modified
    with further options (--su,--posunit,--printall).

    Inspecting the contents of the data file
    ----------------------------------------

    There are also two diagnostic commands: --fieldlist and --idlist
    that can be used to quickly see what data is stored in the
    file. (If no further --range or --fields commands are given, then
    it is this data that is processed by --dump and --vtk).


    Output units
    -----------

     - default is that fields are exported (to vtk file and screen)
       in SI units

     - by default, positions (of mesh nodes) are exported as provided
       by the user through the mesh file. (I.e. if you import a mesh
       where the distance 1 corresponds to 1 nm, then nmagpp will
       output a mesh (for vtk) where the distance 1 corresponds to
       1 nm).

     - positions can be expressed in any unit using the --posunit option

     - fields can be expressed in simulation units using the --su switch.

    """

    version="$Header$"

    parser = optparse.OptionParser(usage=usage,version=version)

    parser.add_option("--idlist",help="dump id list (i.e. the ids of the configurations stored it the file). For each id, the names of the fields that have been stored are displayed.",action="store_true",dest="idlist")

    #parser.add_option("--idlist2",help="dump id list (less detail than --idlist)",action="store_true",dest="idlist2")

    parser.add_option("--fieldlist",help="dump field list (i.e. the names of the fields that are stored in the data file. For each file, show for which ids configurations have been stored)",action="store_true",dest="fl")

    parser.add_option("--fields",help="choose which fields to process (default is all). A list can be provided, for example '--fieds m,phi' will process field 'm' and field 'phi'.",action='store',dest="fields")

    parser.add_option("--range",help="select range of ids to process (default is all). A valid python expression can be given (in quotes), such as --range '[1,4,10]' or --range 'range(10,100,2)'. The expression can also refer to a list with name 'ids' that contains all the ids of the file, so that the last saved configuration can be accessed with --range 'max(ids)'.",action="store",dest="range")

    parser.add_option("--printall",help="if set, all entries (field names and ids) will be printed in commands --idlist and --fieldlist. Otherwise (default) printing of lists is limited to at most 10 entries.",action="store_true",dest="printall")

    parser.add_option("--vtk","-v",help="convert selected ids and their fields to vtk file. For each id a new file will be created and named FILNAME-XXXXXX.vtk where XXXXXX is the id. All fields for that id will be stored in that file",action="store",dest="vtk",metavar="FILENAME.vtk")

    parser.add_option("--vtkascii",help="write vtk file as ascii file (default is binary). Only useful for debugging (the binary default results in smaller file sizes)",\
		       action="store_true",dest="vtkascii",default=False)

    parser.add_option("--su",help="express data in simulation units (default is SI). ",action="store_true",dest="su",default=False)


    parser.add_option("--dump",help="dump data as (ascii) to stdout for all selected ids and their fields",action="store_true",dest="dumpascii")

    parser.add_option("--missing", "-m", help="Create the vtk output files " \
                      "only if they do not exist", action="store_true",
                      dest="missing")

    parser.add_option("--posunit",help="express positions in multiplies of FACTOR metres (overrides --su and --si for the positions)",action="store",dest="posfactor",metavar="FACTOR")
    
    parser.add_option("--loglevel",type="string",metavar="LEVEL",\
                      help="verbosity level (root logger): critical|error|warn|info|info2|debug",\
                      action="store",dest="loglevel")

    (options, arguments) = parser.parse_args(argvs)

    return options,arguments



def short_list(mylist,maxitem=10, formattoken="%2d ", enditems=3):
    """print list mylist, with formattoken for each element, but print at most maxitem elements"""

    global shorten_list

    msg = ""
    if len(mylist) < maxitem or shorten_list==False:
       for item in mylist:
       	   msg += formattoken % item

    else:
	for item in mylist[0:maxitem-enditems]:
	   msg += formattoken  % item
	msg += " ... "
	for item in mylist[-enditems:]:
       	   msg += formattoken  % item
    return msg

def do_dump_field_list(f,fields_by_id):
    """Print fields available in file"""

    ids_by_fieldname = nfem.hdf5_v01.get_saved_ids_by_fieldname(f)

    ids_by_fieldname = {}
    for id in fields_by_id:
        for field in fields_by_id[id]:
            if ids_by_fieldname.has_key(field):
                ids_by_fieldname[field].append(id)
            else:
                ids_by_fieldname[field] =[id]

    for fieldname in ids_by_fieldname:
        ids_by_fieldname[fieldname].sort()
       	print "field %10s (ids=%s)" % (fieldname,short_list(ids_by_fieldname[fieldname]))



def fields2vtkfile( fh, fieldnames, vtkfilename, id, output_unit,
                    options, replace=True):
    """ Currently, scalar and vector data on 2d and 3d meshes is supported.

        Optional extra arguments are:

        :Parameters:

          `fh` : filehandler
            for hdf5 file

          `fieldnames` : list of string
            names of fields to be written to vtk file

          `id` : integer
	    The unique identifier for the configuration to be saved.

          `output_unit` : 'su' or 'si'
            how to express field data (simulation units or SI units)

          `options` : options object from optparse
            (providing flexibility in passing more options to output
            functions).
            In particular, this provides ``options.posfactor`` to scale
            coordinates if requested by user.
	    It also provides ``options.vtkascii`` (bool) which, if true, 
            will result in the vtk file written as an ascii file
            (default is binary)

          `replace`  : bool
            Replace the file, if it exists.
            If replace=False and the file exists, the function
            silently returns.
    """

    if replace and os.path.exists(vtkfilename):
        log.debug("fields2vtkfile: File '%s' exists: exiting!" % vtkfilename)
        return

    if options.vtkascii==True:
	format = 'ascii'
    else:
        format = 'binary'

    log.debug("fields2vtkfile(): Entering, vtkfilename = '%s'" % vtkfilename)
    log.debug("fields2vtkfile(): Fieldlist is %s" % str(fieldnames))
    log.debug("fields2vtkfile(): id is  %s" % id)

    #check that fields are of the right type
    if type(fieldnames) != types.ListType:
        fieldlist = [fieldnames]
    else:
        fieldlist = fieldnames
    if len(fieldlist) == 0:
        raise ValueError, "No field received"

    supoints = fh.root.mesh.points.read()

    points, posunit = _unit_meshposition_conversion_helper(fh,
                                                           options,
                                                           supoints)
    points = points.tolist()

    log.debug("Number of points in mesh=%d" % len(points))

    simplices = fh.root.mesh.simplices.read().tolist()
    log.debug("Number of simplices in mesh=%d" % len(simplices))

    #check dimensionality of the mesh
    if len(points[0])==2:
        log.debug("mesh is 2d")
        mesh_dim = 2
    elif len(points[0])==3:
        log.debug("mesh is 3d")
        mesh_dim = 3
    else:
        log.debug("mesh is 1d")
        mesh_dim = 1

    #assume that 'fieldlist' is a list of FEM fields.
    #First, create the following data structure.
    #dof_to_process = [field, dof, dim]
    dof_to_process = []

    maxind_by_dofname = dict(nfem.hdf5_v01.get_maxind_by_dofname(fh))


    log.debug("Original fieldlist: %s" % fieldlist)

    #reorder field list so that magnetisation comes first. This will
    #make it the default field in Mayavi.
    m_fields = []
    M_fields = []	
    other_fields = []

    for fieldname in fieldlist:
	if fieldname[0:1] == 'm':
	  m_fields.append(fieldname)
	elif fieldname[0:1] == 'M':
	  M_fields.append(fieldname)
        else:
          other_fields.append(fieldname)
 
    #now sort each of the field lists alphabetically and join
    M_fields.sort()
    m_fields.sort()
    other_fields.sort()

    fieldlist = m_fields+M_fields+other_fields

    log.debug("Sorted fieldlist (M,m,other): %s" % fieldlist)	

    for field in fieldlist:
        #learn about dof in this field:
        table = fh.getNode(fh.root.data.fields,field)

        #which dofnames belong to this field?
        dofnames = [colname for colname in table.colnames if not colname in ['time','step','stage','id']]

        for dofname in dofnames:
            maxind = eval(maxind_by_dofname[dofname])

            #print "maxind",maxind
            #print "len(maxind)",len(maxind)
            #print "dofname",dofname
            #print maxind_by_dofname
                
            if len(maxind)>1:
                raise NfemValueError,"Can only deal with scalar and vector data at the moment (no tensor data)."
            if len(maxind)==0:
                dim = 1 #this is a scalar
            elif len(maxind)==1:
                if maxind[0] ==2: #this is a 2d vector
                    dim = 2
                elif maxind[0] ==3: #this is a 3d vector
                    dim = 3
                else:
                    raise NfemValueError,"Can only process vectors in 2 and 3 dimensions"
            dof_to_process.append( (field, dofname, dim) )

            log.debug("adding %s" % str(dof_to_process[-1]) )

    #pyvtk requires us to create a vtk object and to give it the first
    #dof at that time. For all subsequent dofs, we can add them one by one.
    
    #check that we have some dofs to process:
    if len(dof_to_process)==0:
        raise NfemUserError,"Have not found any degrees of freedom to process."

    names = str(map(lambda a : a[1], dof_to_process))
    log.info("About to write field(s) '%s' to %s" % (names,vtkfilename))

    #create vtk object with first data set
    field,dofname, dim = dof_to_process[0]

    #Here we need to have the row per field name information
    row = nfem.hdf5_v01.get_row(fh,field,id)
    log.info("Extracting field %16s from %s at id=%d (that is row=%d), writing to %s" % (field,fh.filename,id,row,vtkfilename))
    tmppos,sidata,tmpsite = nfem.hdf5_v01.get_dof_row_data_in_meshpoint_order(fh,field,dofname,row)

    #check that this is first order basis functino data:
    order_check = tmpsite[0]
    if len(order_check) > 1:
        raise ValueError,"Can only handle 1st order basis function data "\
              "but (%s,%s) seems to be of order %d" % (field,dofname,order_check)

    si_units_dict = nfem.hdf5_v01.get_units_by_dofname(fh)
    si_unit = eval(si_units_dict[dofname])

    data,unit = _unit_conversion_helper( field, dofname, sidata, si_unit, output_unit, options)
    unit_str=unit.dens_str()

    data = data.tolist()  #convert numpy array to list for pyvtk

    vtk = nfem.visual._vtk_createVtkData(points,simplices,data,dofname+unit_str)
    log.log(15,"Adding %d-dim field %s to %s" % (dim,dofname,vtkfilename))

    #and then process all others
    for dof in dof_to_process[1:]:
        field,dofname,dim = dof
	log.debug("About to process field '%s', dof '%s' " % (field,dofname))

        #if only_dofname:
        #    if dofname!=only_dofname:
        #        continue
        log.debug("Adding %d-dim dof %s to %s" % (dim,dofname,vtkfilename))

	#Here we need to have the row per field name information (again)

	row = nfem.hdf5_v01.get_row(fh,field,id)
	log.info("Extracting field %16s from %s at id=%d (that is row=%d), writing to %s" % (field,fh.filename,id,row,vtkfilename))

	tmppos,sidata,tmpsite = nfem.hdf5_v01.get_dof_row_data_in_meshpoint_order(fh,field,dofname,row)
        si_unit = eval(si_units_dict[dofname])
        data,unit = _unit_conversion_helper( field, dofname, sidata, si_unit, output_unit, options)
        unit_str=unit.dens_str()
	data = data.tolist()
        vtk = nfem.visual._vtk_addPointdata(vtk,data,dofname+unit_str)

    vtk.tofile(vtkfilename,format=format)


def do_id_list(f,fields_by_id,verbose=False):
    keys = fields_by_id.keys()
    keys.sort()
    print "%6s  " % "id",
    if verbose:
        print "%4s %6s %9s" % ("stage","step","time"),
    print "%s" % "fields"
    for id in keys:
        print "%6d ->" % id,
        if verbose:
            row,stage,step,time = nfem.hdf5_v01.get_row_stage_step_time_for_field_and_id(f,fields_by_id[id][0],id)
            print "%4d %6d %9.4g" % (stage,step,time),
        print "%s" % short_list(fields_by_id[id],formattoken="%s ")
                                             


def pretty_print_dict(dic):
    keys = dic.keys()
    keys.sort()
    msg = ""
    for key in keys:
        msg += "%5s->%s\n" % (key,dic[key])
    return msg



#main program
options,arguments = parse_command_line(sys.argv)

logging.debug("Options   are: '%s'" % str(options))
logging.debug("Arguments are: '%s'" % str(arguments))

outfile = None
outfile_feature = None

shorten_list = True 

if len(arguments) == 0:
    raise ValueError,"Don't know what this means (expect at least one arguments which is the program name)"
elif len(arguments) == 1:
    raise ValueError,"Need filename of nmag data file to process (use '-h' for help)"
elif len(arguments) in [2,3]:
    infile = arguments[1]
    logging.debug("input file name is '%s'" % infile)

if len(arguments) == 3:
    outfile = arguments[2]
    logging.debug("output file name is '%s'" % outfile)
    
if len(arguments) >= 4:
    print "Arguments are:",arguments
    print "Options   are:",options
    raise ValueError,"It appears you are passing more arguments than I can handle."

#determine infile:
#If given filename exists, use that
if os.path.exists(infile):
    pass
#otherwise try to append _dat.h5
elif os.path.exists(infile+'_dat.h5'):
    infile = infile+'_dat.h5'
else:
    raise ValueError,"Input file '%s' does not exist" % infile

log.log(15,"Input file is %s" % infile)

#now process any requests
def check_and_tag_outfile(feature):
    global outfile,outfile_feature
    if outfile==None:
        raise ValueError,"No output file name has been provided (for feature '%s')." % feature
    if outfile_feature:
        raise ValueError,"Outfile is used already for feature '%s'" % outfile_feature
    outfile_feature = feature
    logging.debug("Will use outfile='%s' for feature '%s'" % (outfile,outfile_feature))


if options.loglevel:
    import nsim
    nsim.logtools.setGlobalLogLevel( options.loglevel )

tables = nfem.hdf5_v01.importtables()
log.info("Opening %s " % infile)
fh = tables.openFile(infile,'r')

nfem.hdf5_v01.checktag(fh,'nsimdata','0.1')


#Read available fields, and create fields_by_id

log.info("Reading metadata from %s " % infile)
fields_by_id = nfem.hdf5_v01.get_saved_fields_by_id(fh)

log.debug("Fields_by_id from file\n"+pretty_print_dict(fields_by_id))

if options.printall:
    log.info("Found '--printall' switch: will print all lists completely")
    shorten_list = False

if options.fields:
    fields = parse_fields_string(options.fields)

    log.info("requested fields are = %r" % repr(fields))
    #reduce fields_by_id by taking out any fields not provided by the user
    new_fields_by_id = {}
    for id in fields_by_id:
        for field in fields_by_id[id]:
            if field in fields:
                if new_fields_by_id.has_key(id):
                    new_fields_by_id[id].append(field)
                else:
                    new_fields_by_id[id] = [field]
    fields_by_id = new_fields_by_id

    log.debug("Fields_by_id after parsing --fields\n"+pretty_print_dict(fields_by_id))
else:
    fields = [] 


if options.range:
    user_range = parse_range_string(options.range,fields_by_id.keys())

    log.info("available id-range = %r" % short_list(fields_by_id.keys(),maxitem=30, formattoken="%2d ", enditems=3))
    log.log(15,"available id-range (all) = %r" % fields_by_id.keys())	
    log.info("requested id-range = %r" % repr(user_range))
    
    #reduce available time steps to what user requested

    new_fields_by_id = {}
    for id in fields_by_id:
        if id in user_range:
            new_fields_by_id[id] = fields_by_id[id]
    fields_by_id = new_fields_by_id

    log.info("id-range to be processed: %s" % repr(short_list(fields_by_id.keys())))

    log.debug("Fields_by_id after parsing --fields\n"+pretty_print_dict(fields_by_id))    


#in what units should we produce output
if options.su:
    output_unit = 'su'
    log.log(15,"Writing data in simulation units")
else:
    output_unit = 'si'
    log.log(15,"Writing data in SI units")

if options.idlist:
    do_id_list(fh,fields_by_id,verbose=True)

#if options.idlist2:
#    do_id_list(fh,fields_by_id,verbose=False)

if options.fl:
    do_dump_field_list(fh,fields_by_id)

if options.dumpascii:
    dump_fields_ascii(fh,fields_by_id,output_unit,options)

if options.vtk:

    ids = fields_by_id.keys()
    ids.sort()

    for id in ids:
        filename=options.vtk
        if filename.endswith(".vtk"):
            filename = filename[:-4]
        filename = "%s-%06d.vtk" % (filename,id)

        fields2vtkfile(fh, fields_by_id[id], filename, id, 
                       output_unit, options, replace=options.missing)

if outfile_feature == None and outfile!=None:
    logging.warn("Outfile name provided but not used ('%s')" % outfile)
